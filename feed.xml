<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.1">Jekyll</generator><link href="https://blog.karun.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.karun.me/" rel="alternate" type="text/html" /><updated>2021-07-19T22:38:19+05:30</updated><id>https://blog.karun.me/feed.xml</id><title type="html">Ramblings of a Coder’s Mind</title><author><name>Karun Japhet</name></author><entry><title type="html">Data storage patterns, versioning and partitions</title><link href="https://blog.karun.me/blog/2021/05/09/data-storage-patterns-versioning-and-partitions/" rel="alternate" type="text/html" title="Data storage patterns, versioning and partitions" /><published>2021-05-09T00:00:00+05:30</published><updated>2021-05-09T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2021/05/09/data-storage-patterns-versioning-and-partitions</id><content type="html" xml:base="https://blog.karun.me/blog/2021/05/09/data-storage-patterns-versioning-and-partitions/">&lt;p&gt;When you have large volumes of data, storing it logically helps users discover information and makes understanding the information easier. In this post, we talk about some of the techniques we use to do so in our application.&lt;/p&gt;

&lt;p&gt;In this post, we are going to use the terminology of AWS S3 buckets to store information. The same techniques can be applied on other cloud, non cloud providers and bare metal servers. Most setups will include a high bandwidth low latency network attached storage with proximity to the processing cluster or disks on HDFS if the entire platform uses HDFS. Your mileage may vary based on your team’s setup and use case. We are also going to talk about techniques which have allowed us to efficiently process this information using Apache Spark as our processing engine. Similar techniques are available for other data processing engines.&lt;/p&gt;

&lt;h1 id=&quot;managing-storage-on-disk&quot;&gt;Managing storage on disk&lt;/h1&gt;

&lt;p&gt;When you have large volumes of data, we have found it useful to separate data that comes in from the upstream providers (if any) from any insights we process and produce. This allows us to segregate access (different parts have different PII classifications) and apply different retention policies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/data-seggregation-using-buckets.png&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/data-seggregation-using-buckets-622x422.png&quot; alt=&quot;Data processing pipeline between various buckets and the operations performed when data moves from one bucket to the other&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We would separate each of these datasets so it’s clear where each came from. When setting up the location to store your data, refer to local laws (like GDPR) for details on data residency requirements.&lt;/p&gt;

&lt;h2 id=&quot;provider-buckets&quot;&gt;Provider buckets&lt;/h2&gt;

&lt;p&gt;Providers tend to make their own directories to send us data. This allows them to have access over how long they want to retain data or if they need to modify information. Data is rarely modified but when it is, a heads up is given to re-process information.&lt;/p&gt;

&lt;p&gt;If this was an event driven system, we would have different event types suggesting that the data from an earlier date was modified. Since the volume of data is large and the batch nature of data transfer on our platform, verbal/written communication is preferred by our data providers which allows us to re-trigger our data pipelines for the affected days.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/provider-buckets-data-layout.png&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/provider-buckets-data-layout-650x373.png&quot; alt=&quot;The preferred layout of provider buckets&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;landing-bucket&quot;&gt;Landing bucket&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/landing-bucket-data-layout.png&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/landing-bucket-data-layout-650x537.png&quot; alt=&quot;Landing bucket data layout&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Most data platforms either procure data or produce it internally. The usual mechanism is for a provider to write data into its own bucket and give its consumers (our platform) access. We copy the data into a landing bucket. This data is a full replica of what the provider gives us without any processing. Keeping data we received from the provider separate from data we process and insights we derive allows us to&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ensure that we don’t accidentally share raw data with others (we are contractually obligated not to share source data)&lt;/li&gt;
  &lt;li&gt;Apply different access policies to raw data when it contains any PII&lt;/li&gt;
  &lt;li&gt;Preserve an untouched copy of the source if we ever have to re-process the data (providers delete data from their bucket within a month or so)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;core-bucket&quot;&gt;Core bucket&lt;/h2&gt;

&lt;p&gt;The data in the landing bucket might be in a format sub optimal for processing (like CSV). The data might also be dirty. We take this opportunity to clean up the data and change the format to something more suitable for processing. For our use case, a downstream pipeline usually consumes a part of what the upstream pipeline produces. Since only a subset of the data is read downstream by a single job, using a file format that allows optimized columnar reads helped us boost performance and thus we use formats like ORC and parquet in our system. The output after this cleanup and transformation is written to the core bucket (since this data is clean input that’s optimised for further processing and thus core to the functioning of the platform).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/core-bucket-data-layout.png&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/core-bucket-data-layout-650x757.png&quot; alt=&quot;Core bucket data layout&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While landing has an exact replica of what the data provider gave us, core’s raw data just transforms it to a more appropriate format (parquet/ORC for our use case) and processing applies some data cleanup strategies, adds meta-data and a few processed columns.&lt;/p&gt;

&lt;h2 id=&quot;derived-bucket&quot;&gt;Derived bucket&lt;/h2&gt;

&lt;p&gt;Your data platform probably has multiple models running on top of the core data that produce multiple insights. We write the output for each of these into its own directory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/derived-bucket-data-layout.png&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/derived-bucket-data-layout-650x1312.png&quot; alt=&quot;Derived bucket data layout&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;advantages-of-data-segregation&quot;&gt;Advantages of data segregation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Separating the data makes it easier to find the data. When you have terabytes or petabytes of information across your organization with multiple teams working on this data platform, it becomes easy to lose track of the information that is already available and it can be hard to find it if they are stored in different places. Having some way to find information is helpful. For us, separating the data by whether we get it from an upstream system, we produce it or we send it out to a downstream system helps teams find information easily.&lt;/li&gt;
  &lt;li&gt;Different rules apply to different datasets. You might be obligated to delete data from raw information you have purchased under certain conditions (like when they have PII). Rules for retaining derived data are different if it does not contain any PII.&lt;/li&gt;
  &lt;li&gt;Most platforms allow archiving of data. Separating the dataset makes it easier to archive different datasets. (we’ll talk about other aspects of archiving during data partitioning)&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;data-partitioning&quot;&gt;Data partitioning&lt;/h1&gt;

&lt;p&gt;Partitioning is a technique that allows your processing engine (like Spark) to read data more efficiently thus making the program more efficient. The most optimal way to partition data is based on the way it is read, written and/or processed. Since most data is written once and read multiple times, optimising a dataset for reads makes sense.&lt;/p&gt;

&lt;p&gt;We create a core bucket for each region we operate in (based on data residency laws of the area). For example, since the EU data cannot leave the EU, we create a derived-bucket in one of the regions in the EU. Under this bucket, we separate the data based on the country, the model that’s producing the data, a version of the data (based on its schema) and the date partition based on which the data was created.&lt;/p&gt;

&lt;p&gt;Reading data from a path like &lt;code&gt;derived-bucket/country=uk/model=alpha/version=1.0&lt;/code&gt; will give you a data set with columns year, month and day. This is useful when you are looking for data across different dates. When filtering the data based on a certain month, frameworks like spark allow the use of &lt;a href=&quot;https://medium.com/inspiredbrilliance/spark-optimization-techniques-a192e8f7d1e4&quot;&gt;push down predicates&lt;/a&gt; making reads more efficient.&lt;/p&gt;

&lt;h1 id=&quot;data-versioning&quot;&gt;Data versioning&lt;/h1&gt;

&lt;p&gt;We change the version of the data every time there is a breaking change. Our versioning strategy is similar to the one talked about in the book for &lt;a href=&quot;https://www.databaserefactoring.com/&quot;&gt;Database Refactoring&lt;/a&gt; with a few changes for scale. The book talks about many types of refactoring and the &lt;a href=&quot;http://www.agiledata.org/essays/renameColumn.html&quot;&gt;column rename&lt;/a&gt; is a common and interesting use case.&lt;/p&gt;

&lt;p&gt;Since the data volume is comparatively low in databases (megabytes to gigabytes), migrating everything to the latest schema is (comparatively) inexpensive. It is important to make sure the application is usable at all points and that there is no point at which the application is not usable.&lt;/p&gt;

&lt;h2 id=&quot;versioning-on-large-data-sets&quot;&gt;Versioning on large data sets&lt;/h2&gt;

&lt;p&gt;When the data volume is high (think terabytes to petabytes), running migrations like this is a very expensive process in terms of the time and resources taken. Also, the application downtime during the migration is large or there’s 2 copies of the dataset created (which makes storage more expensive).&lt;/p&gt;

&lt;h3 id=&quot;non-breaking-schema-changes&quot;&gt;Non breaking schema changes&lt;/h3&gt;

&lt;p&gt;Let’s say you have a dataset that maps the real names to superhero names that you have written to &lt;code&gt;model=superhero-identities/year=2021/month=05/day=01&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;+--------------+-----------------+
|  real_name   | superhero_name  |
+--------------+-----------------+
| Tony Stark   | Iron Man        |
| Steve Rogers | Captain America |
+--------------+-----------------+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next day, if you would like to add their home location, you can write the following data set to the directory &lt;code&gt;day=02&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;+------------------+----------------+--------------------------+
|    real_name     | superhero_name |      home_location       |
+------------------+----------------+--------------------------+
| Bruce Banner     | Hulk           | Dayton, Ohio             |
| Natasha Romanoff | Black Widow    | Stalingrad, Soviet Union |
+------------------+----------------+--------------------------+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Soon after, you realize that storing the real name is too risky. The data you have already published was public knowledge but moving forward, you would like to stop publishing real names. Thus on &lt;code&gt;day=03&lt;/code&gt;, you remove the &lt;code&gt;real_name&lt;/code&gt; column.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;+----------------+---------------------------+
| superhero_name |       home_location       |
+----------------+---------------------------+
| Spider-Man     | Queens, New York          |
| Ant-Man        | San Francisco, California |
+----------------+---------------------------+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When you read &lt;code&gt;derived-bucket/country=uk/model=superhero-identities/&lt;/code&gt; using spark, the framework will read the first schema and use it to read the entire dataset. As a result, you do not see the new &lt;code&gt;home_location&lt;/code&gt; column.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;scala&amp;gt; spark.read.
  parquet(&amp;quot;model=superhero-identities&amp;quot;).
  show()
+----------------+---------------+----+-----+---+
|       real_name| superhero_name|year|month|day|
+----------------+---------------+----+-----+---+
|Natasha Romanoff|    Black Widow|2021|    5|  2|
|    Bruce Banner|           Hulk|2021|    5|  2|
|            null|        Ant-Man|2021|    5|  3|
|            null|     Spider-Man|2021|    5|  3|
|    Steve Rogers|Captain America|2021|    5|  1|
|      Tony Stark|       Iron Man|2021|    5|  1|
+----------------+---------------+----+-----+---+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Asking Spark to merge the schema for you shows all columns (with missing values shown as &lt;code&gt;null&lt;/code&gt;)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;scala&amp;gt; spark.read.option(&amp;quot;mergeSchema&amp;quot;, &amp;quot;true&amp;quot;).
  parquet(&amp;quot;model=superhero-identities&amp;quot;).
  show()
+----------------+---------------+--------------------+----+-----+---+
|       real_name| superhero_name|       home_location|year|month|day|
+----------------+---------------+--------------------+----+-----+---+
|Natasha Romanoff|    Black Widow|Stalingrad, Sovie...|2021|    5|  2|
|    Bruce Banner|           Hulk|        Dayton, Ohio|2021|    5|  2|
|            null|        Ant-Man|San Francisco, Ca...|2021|    5|  3|
|            null|     Spider-Man|    Queens, New York|2021|    5|  3|
|    Steve Rogers|Captain America|                null|2021|    5|  1|
|      Tony Stark|       Iron Man|                null|2021|    5|  1|
+----------------+---------------+--------------------+----+-----+---+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As your model’s schema evolves, using features like merge schema allows you to read the available data across various partitions and then process it. While we have showcased spark’s abilities to merge schemas for parquet files, such capabilities are also available with other file formats.&lt;/p&gt;

&lt;h3 id=&quot;breaking-changes-or-parallel-runs&quot;&gt;Breaking changes or parallel runs&lt;/h3&gt;

&lt;p&gt;Sometimes, you evolve and improve your model. It is useful to do &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallel_running&quot;&gt;parallel runs&lt;/a&gt; and compare the result to verify that it is indeed better before the business switches to use the newer version.&lt;/p&gt;

&lt;p&gt;In such cases we bump up the version of the solution. Let’s assume job alpha v1.0.36 writes to the directory &lt;code&gt;derived-bucket/country=uk/model=alpha/version=1.0&lt;/code&gt;. When we have a newer version of the model (that either has a very different schema or has to be run in parallel), we bump the version of the job (and the location it writes to) to 2.0 making the job alpha v2.0.0 and it’s output directory &lt;code&gt;derived-bucket/country=uk/model=alpha/version=2.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If this change was made and deployed on 1st of Feb and this job runs daily, the latest date partition under &lt;code&gt;model=alpha/version=1.0&lt;/code&gt; will be &lt;code&gt;year=2020/month=01/day=31&lt;/code&gt;. From the 1st of Feb, all data will be written to the &lt;code&gt;model=alpha/version=2.0&lt;/code&gt; directory. If the data in version 2.0 is not sufficient for the business on 1st Feb, we either run backfill jobs to get more data under this partition or we run both version 1 and 2 until version 2’s data is ready to be used by the business.&lt;/p&gt;

&lt;p&gt;The version on disk represents the version of the schema and can be matched up with the versioning of the artifact when using &lt;a href=&quot;https://semver.org&quot;&gt;Semantic Versioning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Each version partition on disk has the same schema (making reads easier)&lt;/li&gt;
  &lt;li&gt;Downstream systems can choose when to migrate from one version to another&lt;/li&gt;
  &lt;li&gt;A new version can be tested out without affecting the existing data pipeline chain&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Applications, system architecture and your data &lt;a href=&quot;https://evolutionaryarchitecture.com/&quot;&gt;always evolve&lt;/a&gt;. Your decisions in how you store and access your data affect your system’s ability to evolve. Using techniques like versioning and partitioning helps your system continue to evolve with minimal overhead cost. Thus, we recommend integrating these techniques into your product at its inception so the team has a strong foundation to build upon.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;https://www.linkedin.com/in/sanjoyb/&quot;&gt;Sanjoy&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anaynayak/&quot;&gt;Anay&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/sathishmandapaka/&quot;&gt;Sathish&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jayant-p/&quot;&gt;Jayant&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/priyaaank/&quot;&gt;Priyank&lt;/a&gt; for their draft reviews and early feedback. Thanks to &lt;a href=&quot;https://www.linkedin.com/in/nikita-oliver/&quot;&gt;Niki&lt;/a&gt; for using her artwork wizardry skills.&lt;/em&gt;&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Techniques" /><category term="data engineering" /><category term="data pipelines" /><category term="big data" /><category term="data storage" /><category term="data partitioning" /><summary type="html">When you have large volumes of data, storing it logically helps users discover information and makes understanding the information easier. In this post, we talk about some of the techniques we use to do so in our application. In this post, we are going to use the terminology of AWS S3 buckets to store information. The same techniques can be applied on other cloud, non cloud providers and bare metal servers. Most setups will include a high bandwidth low latency network attached storage with proximity to the processing cluster or disks on HDFS if the entire platform uses HDFS. Your mileage may vary based on your team’s setup and use case. We are also going to talk about techniques which have allowed us to efficiently process this information using Apache Spark as our processing engine. Similar techniques are available for other data processing engines. Managing storage on disk When you have large volumes of data, we have found it useful to separate data that comes in from the upstream providers (if any) from any insights we process and produce. This allows us to segregate access (different parts have different PII classifications) and apply different retention policies. We would separate each of these datasets so it’s clear where each came from. When setting up the location to store your data, refer to local laws (like GDPR) for details on data residency requirements. Provider buckets Providers tend to make their own directories to send us data. This allows them to have access over how long they want to retain data or if they need to modify information. Data is rarely modified but when it is, a heads up is given to re-process information. If this was an event driven system, we would have different event types suggesting that the data from an earlier date was modified. Since the volume of data is large and the batch nature of data transfer on our platform, verbal/written communication is preferred by our data providers which allows us to re-trigger our data pipelines for the affected days. Landing bucket Most data platforms either procure data or produce it internally. The usual mechanism is for a provider to write data into its own bucket and give its consumers (our platform) access. We copy the data into a landing bucket. This data is a full replica of what the provider gives us without any processing. Keeping data we received from the provider separate from data we process and insights we derive allows us to Ensure that we don’t accidentally share raw data with others (we are contractually obligated not to share source data) Apply different access policies to raw data when it contains any PII Preserve an untouched copy of the source if we ever have to re-process the data (providers delete data from their bucket within a month or so) Core bucket The data in the landing bucket might be in a format sub optimal for processing (like CSV). The data might also be dirty. We take this opportunity to clean up the data and change the format to something more suitable for processing. For our use case, a downstream pipeline usually consumes a part of what the upstream pipeline produces. Since only a subset of the data is read downstream by a single job, using a file format that allows optimized columnar reads helped us boost performance and thus we use formats like ORC and parquet in our system. The output after this cleanup and transformation is written to the core bucket (since this data is clean input that’s optimised for further processing and thus core to the functioning of the platform). While landing has an exact replica of what the data provider gave us, core’s raw data just transforms it to a more appropriate format (parquet/ORC for our use case) and processing applies some data cleanup strategies, adds meta-data and a few processed columns. Derived bucket Your data platform probably has multiple models running on top of the core data that produce multiple insights. We write the output for each of these into its own directory. Advantages of data segregation Separating the data makes it easier to find the data. When you have terabytes or petabytes of information across your organization with multiple teams working on this data platform, it becomes easy to lose track of the information that is already available and it can be hard to find it if they are stored in different places. Having some way to find information is helpful. For us, separating the data by whether we get it from an upstream system, we produce it or we send it out to a downstream system helps teams find information easily. Different rules apply to different datasets. You might be obligated to delete data from raw information you have purchased under certain conditions (like when they have PII). Rules for retaining derived data are different if it does not contain any PII. Most platforms allow archiving of data. Separating the dataset makes it easier to archive different datasets. (we’ll talk about other aspects of archiving during data partitioning) Data partitioning Partitioning is a technique that allows your processing engine (like Spark) to read data more efficiently thus making the program more efficient. The most optimal way to partition data is based on the way it is read, written and/or processed. Since most data is written once and read multiple times, optimising a dataset for reads makes sense. We create a core bucket for each region we operate in (based on data residency laws of the area). For example, since the EU data cannot leave the EU, we create a derived-bucket in one of the regions in the EU. Under this bucket, we separate the data based on the country, the model that’s producing the data, a version of the data (based on its schema) and the date partition based on which the data was created. Reading data from a path like derived-bucket/country=uk/model=alpha/version=1.0 will give you a data set with columns year, month and day. This is useful when you are looking for data across different dates. When filtering the data based on a certain month, frameworks like spark allow the use of push down predicates making reads more efficient. Data versioning We change the version of the data every time there is a breaking change. Our versioning strategy is similar to the one talked about in the book for Database Refactoring with a few changes for scale. The book talks about many types of refactoring and the column rename is a common and interesting use case. Since the data volume is comparatively low in databases (megabytes to gigabytes), migrating everything to the latest schema is (comparatively) inexpensive. It is important to make sure the application is usable at all points and that there is no point at which the application is not usable. Versioning on large data sets When the data volume is high (think terabytes to petabytes), running migrations like this is a very expensive process in terms of the time and resources taken. Also, the application downtime during the migration is large or there’s 2 copies of the dataset created (which makes storage more expensive). Non breaking schema changes Let’s say you have a dataset that maps the real names to superhero names that you have written to model=superhero-identities/year=2021/month=05/day=01. +--------------+-----------------+ | real_name | superhero_name | +--------------+-----------------+ | Tony Stark | Iron Man | | Steve Rogers | Captain America | +--------------+-----------------+ The next day, if you would like to add their home location, you can write the following data set to the directory day=02. +------------------+----------------+--------------------------+ | real_name | superhero_name | home_location | +------------------+----------------+--------------------------+ | Bruce Banner | Hulk | Dayton, Ohio | | Natasha Romanoff | Black Widow | Stalingrad, Soviet Union | +------------------+----------------+--------------------------+ Soon after, you realize that storing the real name is too risky. The data you have already published was public knowledge but moving forward, you would like to stop publishing real names. Thus on day=03, you remove the real_name column. +----------------+---------------------------+ | superhero_name | home_location | +----------------+---------------------------+ | Spider-Man | Queens, New York | | Ant-Man | San Francisco, California | +----------------+---------------------------+ When you read derived-bucket/country=uk/model=superhero-identities/ using spark, the framework will read the first schema and use it to read the entire dataset. As a result, you do not see the new home_location column. scala&amp;gt; spark.read. parquet(&amp;quot;model=superhero-identities&amp;quot;). show() +----------------+---------------+----+-----+---+ | real_name| superhero_name|year|month|day| +----------------+---------------+----+-----+---+ |Natasha Romanoff| Black Widow|2021| 5| 2| | Bruce Banner| Hulk|2021| 5| 2| | null| Ant-Man|2021| 5| 3| | null| Spider-Man|2021| 5| 3| | Steve Rogers|Captain America|2021| 5| 1| | Tony Stark| Iron Man|2021| 5| 1| +----------------+---------------+----+-----+---+ Asking Spark to merge the schema for you shows all columns (with missing values shown as null) scala&amp;gt; spark.read.option(&amp;quot;mergeSchema&amp;quot;, &amp;quot;true&amp;quot;). parquet(&amp;quot;model=superhero-identities&amp;quot;). show() +----------------+---------------+--------------------+----+-----+---+ | real_name| superhero_name| home_location|year|month|day| +----------------+---------------+--------------------+----+-----+---+ |Natasha Romanoff| Black Widow|Stalingrad, Sovie...|2021| 5| 2| | Bruce Banner| Hulk| Dayton, Ohio|2021| 5| 2| | null| Ant-Man|San Francisco, Ca...|2021| 5| 3| | null| Spider-Man| Queens, New York|2021| 5| 3| | Steve Rogers|Captain America| null|2021| 5| 1| | Tony Stark| Iron Man| null|2021| 5| 1| +----------------+---------------+--------------------+----+-----+---+ As your model’s schema evolves, using features like merge schema allows you to read the available data across various partitions and then process it. While we have showcased spark’s abilities to merge schemas for parquet files, such capabilities are also available with other file formats. Breaking changes or parallel runs Sometimes, you evolve and improve your model. It is useful to do parallel runs and compare the result to verify that it is indeed better before the business switches to use the newer version. In such cases we bump up the version of the solution. Let’s assume job alpha v1.0.36 writes to the directory derived-bucket/country=uk/model=alpha/version=1.0. When we have a newer version of the model (that either has a very different schema or has to be run in parallel), we bump the version of the job (and the location it writes to) to 2.0 making the job alpha v2.0.0 and it’s output directory derived-bucket/country=uk/model=alpha/version=2.0. If this change was made and deployed on 1st of Feb and this job runs daily, the latest date partition under model=alpha/version=1.0 will be year=2020/month=01/day=31. From the 1st of Feb, all data will be written to the model=alpha/version=2.0 directory. If the data in version 2.0 is not sufficient for the business on 1st Feb, we either run backfill jobs to get more data under this partition or we run both version 1 and 2 until version 2’s data is ready to be used by the business. The version on disk represents the version of the schema and can be matched up with the versioning of the artifact when using Semantic Versioning. Advantages Each version partition on disk has the same schema (making reads easier) Downstream systems can choose when to migrate from one version to another A new version can be tested out without affecting the existing data pipeline chain Summary Applications, system architecture and your data always evolve. Your decisions in how you store and access your data affect your system’s ability to evolve. Using techniques like versioning and partitioning helps your system continue to evolve with minimal overhead cost. Thus, we recommend integrating these techniques into your product at its inception so the team has a strong foundation to build upon. Thanks to Sanjoy, Anay Sathish, Jayant and Priyank for their draft reviews and early feedback. Thanks to Niki for using her artwork wizardry skills.</summary></entry><entry><title type="html">Version controlled configuration and secrets management for Terraform</title><link href="https://blog.karun.me/blog/2019/08/26/version-controlled-configuration-and-secrets-management-for-terraform/" rel="alternate" type="text/html" title="Version controlled configuration and secrets management for Terraform" /><published>2019-08-26T00:00:00+05:30</published><updated>2019-08-26T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2019/08/26/version-controlled-configuration-and-secrets-management-for-terraform</id><content type="html" xml:base="https://blog.karun.me/blog/2019/08/26/version-controlled-configuration-and-secrets-management-for-terraform/">&lt;p&gt;&lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt; is a tool to build your infrastructure as code. We’ve been having a few challenges while trying to figure out how to how to manage configuration and secrets when integrating terraform with our CD pipeline.&lt;/p&gt;

&lt;!-- more --&gt;
&lt;h2 id=&quot;life-before-version-control&quot;&gt;Life before version control&lt;/h2&gt;
&lt;p&gt;Before we can do that, it’s important to understand build process before we began on this journey.
&lt;a href=&quot;https://blog.karun.me/assets/images/uploads/terraform-environments.jpg&quot;&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/terraform-environments.jpg&quot; alt=&quot;Terraform managed environments&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our build model for this project was branch based. Each environment maps to a branch (&lt;code&gt;main -&amp;gt; dev&lt;/code&gt;, &lt;code&gt;uat -&amp;gt; uat&lt;/code&gt; and &lt;code&gt;production -&amp;gt; production&lt;/code&gt;). All other (feature) branches only ran the plan stage against the &lt;code&gt;dev&lt;/code&gt; environment.&lt;/p&gt;

&lt;p&gt;As you can notice, the configurations, secrets and keys are all maintained on the build agent. This means, every developer wanting to run plan and test their changes needs to replicate the &lt;code&gt;terraform_variables&lt;/code&gt; directory. Any mistakes in doing so masks actual issues that your pipeline might face leading to delayed feedback.&lt;/p&gt;

&lt;p&gt;Next, let’s look at what our codebase looked like&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;terraform
├── module-1
│   ├── backend.tf
│   ├── data.tf
│   ├── resources.tf
│   ├── provider.tf
│   └── variables.tf
├── module-2
│   ├── backend.tf
│   ├── data.tf
│   ├── resources.tf
│   ├── provider.tf
│   └── variables.tf
└── scripts
    └── provision
        ├── apply.sh
        ├── init.sh
        └── plan.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The provisioning scripts help us consistently run different stages across modules. Each module is an independent area of our infrastructure (such as core networking, HTTP services etc.)&lt;/p&gt;

&lt;p&gt;Each of the provisioning scripts accepted a &lt;code&gt;WORKSPACE_NAME&lt;/code&gt; (branch for execution that maps to the environment terraform is running for) and &lt;code&gt;MODULE_NAME&lt;/code&gt; (module being executed).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;init.sh&lt;/code&gt; ran the &lt;code&gt;terraform init&lt;/code&gt; stage of the pipeline downloading the necessary plugins and initializing the backend&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

cd $MODULE_NAME

echo &amp;quot;init default.tfstate&amp;quot;
terraform init -backend-config=&amp;quot;key=default.tfstate&amp;quot;

echo &amp;quot;select or create new workspace $WORKSPACE_NAME&amp;quot;
terraform workspace select $WORKSPACE_NAME || terraform workspace new $WORKSPACE_NAME

echo &amp;quot;init $MODULE_NAME/terraform.tfstate&amp;quot;
terraform init -backend-config=&amp;quot;key=$MODULE_NAME/terraform.tfstate&amp;quot; -force-copy -reconfigure&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/52c755ca164de009f1e37bebfdac46ea.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;code&gt;plan.sh&lt;/code&gt; ran the &lt;code&gt;terraform plan&lt;/code&gt; stage allowing users to review their changes before applying them.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

cd $MODULE_NAME

echo &amp;quot;select or create new workspace $WORKSPACE_NAME&amp;quot;
terraform workspace select $WORKSPACE_NAME || terraform workspace new $WORKSPACE_NAME

echo &amp;quot;plan with var file ~/terraform_variables/$WORKSPACE_NAME/$MODULE_NAME.tfvars&amp;quot;
terraform plan -var-file=~/terraform_variables/$WORKSPACE_NAME/$MODULE_NAME.tfvars -out=$MODULE_NAME.tfplan -input=false&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/5ab158cd0aa7ba492872cff7061d8814.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;code&gt;apply.sh&lt;/code&gt; applied the changes onto an environment. Developers do not run this command from local to ensure consistency on the environment&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

cd $MODULE_NAME

echo &amp;quot;select or create new workspace $WORKSPACE_NAME&amp;quot;
terraform workspace select $WORKSPACE_NAME || terraform workspace new $WORKSPACE_NAME

echo &amp;quot;apply with var file ~/terraform_variables/$WORKSPACE_NAME/$MODULE_NAME.tfvars&amp;quot;
terraform apply -var-file=~/terraform_variables/$WORKSPACE_NAME/$MODULE_NAME.tfvars -auto-approve&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/48795c981c5495d38184a4cf52a1cd2c.js&quot;&gt; &lt;/script&gt;

&lt;h2 id=&quot;version-controlling-configuration&quot;&gt;Version controlling configuration&lt;/h2&gt;
&lt;p&gt;We moved the variables into the &lt;code&gt;config&lt;/code&gt; directory by making a directory for every branch for each of the 3 environments we had.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;terraform
├── config
│   ├── main
│   │   ├── module-1.tfvars
│   │   └── module-2.tfvars
│   ├── production
│   │   ├── module-1.tfvars
│   │   └── module-2.tfvars
│   ├── uat
│   │   ├── module-1.tfvars
│   │   └── module-2.tfvars
├── module-1
│   └── ...
├── module-2
|   └── ...
└── scripts
    ├── provision
    │   ├── apply.sh
    │   ├── functions.sh
    │   ├── init.sh
    │   └── plan.sh
    └── test_variable_names.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;According to &lt;a href=&quot;https://www.terraform.io/docs/configuration/variables.html#environment-variables&quot;&gt;terraform’s documentation&lt;/a&gt;, you can export a variable that your terraform codes need with a prefix of &lt;code&gt;TF_VAR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;functions.sh&lt;/code&gt; provides convenience functions to read the configuration and secrets.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash

function fetch_variables() {
    workspace_name=$1
    module_name=$2

    echo $(cat ../config/$workspace_name/$module_name.tfvars | sed &amp;#39;/^$/D&amp;#39; | sed &amp;#39;s/.*/TF_VAR_&amp;amp; /&amp;#39; | tr -d &amp;#39;\n&amp;#39;)
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/d9314848516e919952fbfc7c681a5488.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;code&gt;fetch_variables&lt;/code&gt; read the &lt;code&gt;tfvars&lt;/code&gt; file, removes empty lines (that were added for readability), prefixed the name with &lt;code&gt;TF_VAR&lt;/code&gt; and joined all entries into a single line. The string this method returns can be used as a prefix to the &lt;code&gt;terraform&lt;/code&gt; command while running &lt;code&gt;plan&lt;/code&gt; and &lt;code&gt;apply&lt;/code&gt; making them environment variables.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Updated plan and apply scripts are placed in the secrets management section for brevity&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;testing-configuration-files&quot;&gt;Testing configuration files&lt;/h3&gt;
&lt;p&gt;The only limitation is that &lt;strong&gt;none of these variables can have a hyphen&lt;/strong&gt; in the name because of &lt;a href=&quot;https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Definitions&quot;&gt;shell variable naming rules&lt;/a&gt;. As with any potential mistake, a test providing feedback helps protect you from run time failures. &lt;code&gt;test_variable_names.sh&lt;/code&gt; does this check for us.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;#!/bin/bash

function parse_and_test_properties_entries() {
    prop=$1
    if [[ &amp;quot;$prop&amp;quot; == &amp;quot;&amp;quot; || $prop = \#* ]]; then
        continue
    fi

    key=&amp;quot;$(cut -d&amp;#39;=&amp;#39; -f1 &amp;lt;&amp;lt;&amp;lt;&amp;quot;$prop&amp;quot;)&amp;quot;
    if [[ $key =~ &amp;quot;-&amp;quot; ]]; then
        echo &amp;quot;$filename contains \&amp;quot;$key\&amp;quot; which contains a hyphen&amp;quot;
        exit 1
    fi
}

function parse_file() {
    filename=$1
    OLD_IFS=$IFS
    props=$(cat $filename)

    IFS=$&amp;#39;\n&amp;#39;
    for prop in ${props[@]}; do
        parse_and_test_properties_entries $prop
    done
    IFS=$OLD_IFS
}

base_dir=&amp;quot;config&amp;quot;
for sub_dir in $(find $base_dir -mindepth 1 -maxdepth 1 -type d); do
    workspace_name=${sub_dir#&amp;quot;$base_dir/&amp;quot;}

    for input_file in config/$workspace_name/*.tfvars; do
        parse_file $input_file
    done

    echo &amp;quot;All variables are named correctly in config/$workspace_name&amp;quot;
done&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/1497470a61c9f06689deaaf19a1610e1.js&quot;&gt; &lt;/script&gt;

&lt;h2 id=&quot;version-controlling-secrets&quot;&gt;Version controlling secrets&lt;/h2&gt;
&lt;p&gt;Secrets like passwords can be version controlled in a similar way though they require encryption to keep them safe. We’re using &lt;a href=&quot;https://www.openssl.org/&quot;&gt;OpenSSL&lt;/a&gt; with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Symmetric-key_algorithm&quot;&gt;symmetric key&lt;/a&gt; to encrypt our secrets. Each secret is put into a &lt;code&gt;tfsecrets&lt;/code&gt; file (internally a property file just like &lt;code&gt;tfvars&lt;/code&gt; files for configuration). When encrypted, the file will have an extension of &lt;code&gt;.tfsecrets.enc&lt;/code&gt;. When the &lt;code&gt;plan&lt;/code&gt; or &lt;code&gt;apply&lt;/code&gt; stages are executed, files are decrypted &lt;strong&gt;in memory&lt;/strong&gt; (and not on disk, for security reasons) and used the same way.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;functions.sh&lt;/code&gt; gets a new addition to support reading all secrets&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;function fetch_secrets() {
    workspace_name=$1
    module_name=$2
    secret_key_for_workspace=$(eval &amp;quot;echo \$SECRET_KEY_$workspace_name&amp;quot;)
    echo $(openssl enc -aes-256-cbc -d -in ../config/$workspace_name/$module_name.tfsecrets.enc -pass pass:$secret_key_for_workspace | sed &amp;#39;/^$/D&amp;#39; | sed &amp;#39;s/.*/TF_VAR_&amp;amp; /&amp;#39; | tr -d &amp;#39;\n&amp;#39;)
}&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/f78a72e02ce9aced0636d61672a2b777.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;The astute amongst you probably noticed that we’re using OpenSSL v1.0.2s because v1.1.x changes the syntax on encryption/decryption of files. Also, you might have noticed the use of environment variables like &lt;code&gt;SECRET_KEY_main&lt;/code&gt;, &lt;code&gt;SECRET_KEY_uat&lt;/code&gt; and &lt;code&gt;SECRET_KEY_production&lt;/code&gt; as the encryption keys. These values are stored on our CI server (in our case &lt;a href=&quot;https://gitlab.com/&quot;&gt;GitLab&lt;/a&gt;) which makes these values available to our CI agent during execution.&lt;/p&gt;

&lt;p&gt;For local development, we have scripts to encrypt and decrypt configuration files either one at a time or in bulk per environment. It’s worth noting that re-encryption of the same file will show up on your &lt;code&gt;git diff&lt;/code&gt; since the encrypted file’s metadata changes. Only check in encrypted files when their contents have changed (helping you debug future issues)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;encrypt.sh&lt;/code&gt; takes &lt;code&gt;SECRET_KEY&lt;/code&gt; as an environment variable for making local usage easier.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

if [ -z &amp;quot;$SECRET_KEY&amp;quot; ]; then
    echo &amp;quot;Set a SECRET_KEY for \&amp;quot;$WORKSPACE_NAME\&amp;quot; encryption&amp;quot;
    exit 1
fi

function encrypt_file() {
    input_file=$1
    target_file=&amp;quot;$input_file.enc&amp;quot;
    echo &amp;quot;Encrypting $input_file to $target_file&amp;quot;
    openssl enc -aes-256-cbc -salt -in $input_file -out $target_file -pass pass:$SECRET_KEY
    rm -f $input_file
}

if [ -z $1 ]; then
    echo &amp;quot;Usage:&amp;quot;
    echo &amp;quot;  ./scripts/encrypt.sh &amp;lt;filePathFromProjectRoot&amp;gt;&amp;quot;
    echo &amp;quot;  ./scripts/encrypt.sh all&amp;quot;
    exit 2
elif [ &amp;quot;$1&amp;quot; == &amp;quot;all&amp;quot; ]; then
    for input_file in config/$WORKSPACE_NAME/*.tfsecrets; do
        encrypt_file $input_file
    done
else
    encrypt_file $1
fi&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/8775d0d2a9ad124eefff9df6b2d431eb.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;code&gt;decrypt.sh&lt;/code&gt; also takes the same &lt;code&gt;SECRET_KEY&lt;/code&gt; as an environment variable for making local usage easier.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

if [ -z &amp;quot;$SECRET_KEY&amp;quot; ]; then
    echo &amp;quot;Set a SECRET_KEY for \&amp;quot;$WORKSPACE_NAME\&amp;quot; decryption&amp;quot;
    exit 1
fi

function decrypt_file() {
    input_file=$1
    target_file=${input_file%&amp;quot;.enc&amp;quot;}
    echo &amp;quot;Decrypting $input_file to $target_file&amp;quot;
    openssl enc -aes-256-cbc -d -in $input_file -out $target_file -pass pass:$SECRET_KEY
    rm -f $input_file
}

if [ -z $1 ]; then
    echo &amp;quot;Usage:&amp;quot;
    echo &amp;quot;  ./scripts/decrypt.sh &amp;lt;filePathFromProjectRoot&amp;gt;&amp;quot;
    echo &amp;quot;  ./scripts/decrypt.sh all&amp;quot;
    exit 2
elif [ &amp;quot;$1&amp;quot; == &amp;quot;all&amp;quot; ]; then
    for input_file in config/$WORKSPACE_NAME/*.tfsecrets.enc
    do
        decrypt_file $input_file
    done
else
    decrypt_file $1
fi&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/f1e33a666587f4ade051e725e196742e.js&quot;&gt; &lt;/script&gt;

&lt;h3 id=&quot;testing-secret-files&quot;&gt;Testing secret files&lt;/h3&gt;
&lt;p&gt;If all files for an environment aren’t checked with the same key, you’ll face a runtime error. Since files can be encrypted individually, you must test if all files have been encrypted correctly. This test is also useful when you’re rotating the &lt;code&gt;SECRET_KEY&lt;/code&gt; for an environment.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;test_encryption.sh&lt;/code&gt; needs &lt;code&gt;SECRET_KEY_&amp;lt;env&amp;gt;&lt;/code&gt; values set so it can be executed locally.&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash

base_dir=&amp;quot;config&amp;quot;

for sub_dir in $(find $base_dir -mindepth 1 -maxdepth 1 -type d); do
    workspace_name=${sub_dir#&amp;quot;$base_dir/&amp;quot;}
    password_var_name=&amp;quot;\$SECRET_KEY_$workspace_name&amp;quot;
    secret_key_for_workspace=$(eval &amp;quot;echo $password_var_name&amp;quot;)

    if [ -z &amp;quot;$secret_key_for_workspace&amp;quot; ]; then
        echo &amp;quot;Variable $password_var_name has not been set. Unable to test&amp;quot;
        exit 1
    fi

    for input_file in config/$workspace_name/*.tfsecrets.enc
    do
        openssl enc -aes-256-cbc -d -in $input_file -pass pass:$secret_key_for_workspace &amp;amp;&amp;gt; /dev/null
        if [ $? != 0 ]; then
            echo &amp;quot;Unable to decrypt $input_file with $password_var_name&amp;quot;
            exit 1
        fi
    done

    echo &amp;quot;Successfully decrypted all secrets in config/$workspace_name&amp;quot;
done&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/5aedf7066b408511975d3cb97ce0ee5a.js&quot;&gt; &lt;/script&gt;

&lt;h3 id=&quot;end-result&quot;&gt;End result&lt;/h3&gt;
&lt;p&gt;Our final project structure contains the following files&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;terraform
├── config
│   ├── main
│   │   ├── module-1.tfvars
│   │   ├── module-1.tfsecrets.enc
│   │   ├── module-2.tfvars
│   │   └── module-2.tfsecrets.enc
│   ├── production
│   │   ├── module-1.tfvars
│   │   ├── module-1.tfsecrets.enc
│   │   ├── module-2.tfvars
│   │   └── module-2.tfsecrets.enc
│   ├── uat
│   │   ├── module-1.tfvars
│   │   ├── module-1.tfsecrets.enc
│   │   ├── module-2.tfvars
│   │   └── module-2.tfsecrets.enc
├── module-1
│   └── ...
├── module-2
|   └── ...
└── scripts
    ├── decrypt.sh
    ├── encrypt.sh
    ├── provision
    │   ├── apply.sh
    │   ├── functions.sh
    │   ├── init.sh
    │   └── plan.sh
    ├── test_encryption.sh
    └── test_variable_names.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;plan.sh&lt;/code&gt; uses &lt;code&gt;functions.sh&lt;/code&gt; to load configuration and secrets&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

source $(dirname &amp;quot;$0&amp;quot;)/functions.sh

cd $MODULE_NAME

echo &amp;quot;select or create new workspace $WORKSPACE_NAME&amp;quot;
terraform workspace select $WORKSPACE_NAME || terraform workspace new $WORKSPACE_NAME

echo &amp;quot;plan with var file config/$WORKSPACE_NAME/$MODULE_NAME.tfvars&amp;quot;
config=$(fetch_variables $WORKSPACE_NAME $MODULE_NAME)
secrets=$(fetch_secrets $WORKSPACE_NAME $MODULE_NAME)
eval &amp;quot;$secrets $config terraform plan -out=$MODULE_NAME.tfplan -input=false&amp;quot;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/3efb6a5d416d4149678b427bc37ff154.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;&lt;code&gt;apply.sh&lt;/code&gt; uses &lt;code&gt;functions.sh&lt;/code&gt; in a similar fashion&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;#!/bin/bash
set -e

source $(dirname &amp;quot;$0&amp;quot;)/functions.sh

cd $MODULE_NAME

echo &amp;quot;select or create new workspace $WORKSPACE_NAME&amp;quot;
terraform workspace select $WORKSPACE_NAME || terraform workspace new $WORKSPACE_NAME

echo &amp;quot;apply with var file config/$WORKSPACE_NAME/$MODULE_NAME.tfvars&amp;quot;
config=$(fetch_variables $WORKSPACE_NAME $MODULE_NAME)
secrets=$(fetch_secrets $WORKSPACE_NAME $MODULE_NAME)
eval &amp;quot;$secrets $config terraform apply -auto-approve&amp;quot;&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/javatarz/8e77d8ee1474a4d06faf9a4cc19ec0df.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;And thus, our terraform project requires no data from the CI agent and can be executed perfectly from any box as long as it has the latest code checked out and the correct version of terraform.&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Tutorial" /><category term="terraform" /><category term="gitlab" /><category term="version control" /><category term="continuous delivery" /><category term="infrastructure as code" /><summary type="html">Terraform is a tool to build your infrastructure as code. We’ve been having a few challenges while trying to figure out how to how to manage configuration and secrets when integrating terraform with our CD pipeline.</summary></entry><entry><title type="html">Managing multiple signatures for git repositories</title><link href="https://blog.karun.me/blog/2019/06/11/managing-multiple-signatures-for-git-repositories/" rel="alternate" type="text/html" title="Managing multiple signatures for git repositories" /><published>2019-06-11T00:00:00+05:30</published><updated>2019-06-11T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2019/06/11/managing-multiple-signatures-for-git-repositories</id><content type="html" xml:base="https://blog.karun.me/blog/2019/06/11/managing-multiple-signatures-for-git-repositories/">&lt;p&gt;Github explains pretty well &lt;a href=&quot;https://help.github.com/en/articles/signing-commits&quot;&gt;how to sign commits&lt;/a&gt;. You can make it automatic by globally setting &lt;code&gt;commit.gpgsign = true&lt;/code&gt; by using&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git config --global commit.gpgsign true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if you have different signatures for your personal ID and your work ID?&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;First, you create multiple signatures. It is important that the &lt;strong&gt;email address in the signature is the same as the one for the user who has authored the commit&lt;/strong&gt;. Run &lt;code&gt;gpg -K --keyid-format SHORT&lt;/code&gt; to see all available keys. The output looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/Users/karun/.gnupg/pubring.kbx
-------------------------------
sec   rsa4096/11111111 2019-06-11 [SC]
      1234567890123456789012345678901211111111
uid         [ultimate] Karun Japhet &amp;lt;karun@personal.com&amp;gt;
ssb   rsa4096/22222222 2019-06-11 [E]

sec   rsa4096/33333333 2019-06-11 [SC]
      0987654321098765432109876543210933333333
uid         [ultimate] Karun Japhet &amp;lt;karunj@work.com&amp;gt;
ssb   rsa4096/44444444 2019-06-11 [E]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fetch the ID for each of the signatures. The ID for the personal signature is 11111111 and that for the work signature is 33333333. To assign a signature to the repo, execute &lt;code&gt;git config user.signingkey &amp;lt;ID&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Personally, I have aliases for personal and work signatures and every time I checkout a project, run the alias once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;alias signpersonal= &quot;git config user.signingkey 11111111 &amp;amp;&amp;amp; git config user.email \&quot;karun@personal.com\&quot;&quot;
alias signwork    = &quot;git config user.signingkey 33333333 &amp;amp;&amp;amp; git config user.email \&quot;karun@work.com\&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;code&gt;git log --show-signature&lt;/code&gt; to verify if a commit used the right signature. Happy commit-signing.&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Tutorial" /><category term="Git" /><category term="Signing" /><summary type="html">Github explains pretty well how to sign commits. You can make it automatic by globally setting commit.gpgsign = true by using git config --global commit.gpgsign true What if you have different signatures for your personal ID and your work ID? First, you create multiple signatures. It is important that the email address in the signature is the same as the one for the user who has authored the commit. Run gpg -K --keyid-format SHORT to see all available keys. The output looks like /Users/karun/.gnupg/pubring.kbx ------------------------------- sec rsa4096/11111111 2019-06-11 [SC] 1234567890123456789012345678901211111111 uid [ultimate] Karun Japhet &amp;lt;karun@personal.com&amp;gt; ssb rsa4096/22222222 2019-06-11 [E] sec rsa4096/33333333 2019-06-11 [SC] 0987654321098765432109876543210933333333 uid [ultimate] Karun Japhet &amp;lt;karunj@work.com&amp;gt; ssb rsa4096/44444444 2019-06-11 [E] Fetch the ID for each of the signatures. The ID for the personal signature is 11111111 and that for the work signature is 33333333. To assign a signature to the repo, execute git config user.signingkey &amp;lt;ID&amp;gt;. Personally, I have aliases for personal and work signatures and every time I checkout a project, run the alias once. alias signpersonal= &quot;git config user.signingkey 11111111 &amp;amp;&amp;amp; git config user.email \&quot;karun@personal.com\&quot;&quot; alias signwork = &quot;git config user.signingkey 33333333 &amp;amp;&amp;amp; git config user.email \&quot;karun@work.com\&quot;&quot; Run git log --show-signature to verify if a commit used the right signature. Happy commit-signing.</summary></entry><entry><title type="html">Fixing broken Social logins on your browser</title><link href="https://blog.karun.me/blog/2019/04/16/fixing-broken-social-logins-on-your-browser/" rel="alternate" type="text/html" title="Fixing broken Social logins on your browser" /><published>2019-04-16T00:00:00+05:30</published><updated>2019-04-16T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2019/04/16/fixing-broken-social-logins-on-your-browser</id><content type="html" xml:base="https://blog.karun.me/blog/2019/04/16/fixing-broken-social-logins-on-your-browser/">&lt;p&gt;Privacy vs Convienience is a constant battle. Personally, I prefer dialing up my privacy up to 11 to avoid being tracked. Every once in a while, &lt;em&gt;social logins&lt;/em&gt; are important because it’s the only way to use a service. If this service is an internal company login that only uses social login via the company’s Google ID, you don’t have much of a chance.&lt;/p&gt;

&lt;p&gt;If your login just won’t work, try changing the following settings&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;privacy-badger&quot;&gt;Privacy Badger&lt;/h2&gt;
&lt;p&gt;Allow calls to &lt;code&gt;accounts.google.com&lt;/code&gt; &amp;amp; &lt;code&gt;apis.google.com&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;firefox-settings&quot;&gt;Firefox settings&lt;/h2&gt;
&lt;p&gt;Allow Third party trackers in Firefox through Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Cookies &amp;gt; Third-party trackers&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Guide" /><category term="Tips" /><category term="Browser" /><category term="Privacy" /><summary type="html">Privacy vs Convienience is a constant battle. Personally, I prefer dialing up my privacy up to 11 to avoid being tracked. Every once in a while, social logins are important because it’s the only way to use a service. If this service is an internal company login that only uses social login via the company’s Google ID, you don’t have much of a chance. If your login just won’t work, try changing the following settings Privacy Badger Allow calls to accounts.google.com &amp;amp; apis.google.com Firefox settings Allow Third party trackers in Firefox through Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Cookies &amp;gt; Third-party trackers</summary></entry><entry><title type="html">The untold guide to troubleshoot Phillips Hue and Google Assistant Integration</title><link href="https://blog.karun.me/blog/2018/11/10/the-untold-guide-to-troubleshoot-phillips-hue-and-google-assistant-integration/" rel="alternate" type="text/html" title="The untold guide to troubleshoot Phillips Hue and Google Assistant Integration" /><published>2018-11-10T00:00:00+05:30</published><updated>2018-11-10T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2018/11/10/the-untold-guide-to-troubleshoot-phillips-hue-and-google-assistant-integration</id><content type="html" xml:base="https://blog.karun.me/blog/2018/11/10/the-untold-guide-to-troubleshoot-phillips-hue-and-google-assistant-integration/">&lt;p&gt;Recently, I moved into a new home and was setting up my &lt;a href=&quot;https://www2.meethue.com/en-us&quot;&gt;Phillips Hue&lt;/a&gt; lights with my &lt;a href=&quot;https://assistant.google.com/#?modal_active=none&quot;&gt;Google Home assistants&lt;/a&gt; around my house for convenience. I noticed a couple of hick-ups since the last time I did this.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;logging-into-phillips-hue-app&quot;&gt;Logging into Phillips Hue app&lt;/h2&gt;
&lt;p&gt;If the app does not ask you to hit the button on your bridge, your account &lt;strong&gt;already has a bridge&lt;/strong&gt; associated with it.&lt;/p&gt;

&lt;p&gt;You can see what bridge is associated with your &lt;a href=&quot;https://account.meethue.com&quot;&gt;MeetHue account&lt;/a&gt; on the &lt;a href=&quot;https://account.meethue.com/bridge&quot;&gt;bridges page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://chapterbreak.net/wp-content/uploads/2015/12/only-one.gif&quot; alt=&quot;There can only be one&quot; title=&quot;There can only be one&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Remove any older bridges you might have on your account and try logging into the Phillips Hue app again. Once complete, you should be able to link your Google Home assistant to your Phillips Hue app.&lt;/p&gt;

&lt;h2 id=&quot;other-house-keeping-for-security-reasons&quot;&gt;Other House Keeping for security reasons&lt;/h2&gt;

&lt;p&gt;You can cleanup how many &lt;a href=&quot;https://account.meethue.com/apps&quot;&gt;apps have access to your account&lt;/a&gt; and &lt;a href=&quot;https://account.meethue.com/bridge&quot;&gt;how many other users have access to your bridge&lt;/a&gt;. If you see anything that your don’t recognize, remove it. After all, these apps and Hue account users can control the lights in your house. If you don’t know them, remove their access.&lt;/p&gt;

&lt;p&gt;In my case, the only users on my bridge are the family members in my house and the only apps I have are the Phillips Hue Android app (for mobile access remotely) and Google (assistant integration).&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Guide" /><category term="IOT" /><category term="smart assistant" /><category term="smart integration" /><category term="Phillips hue" /><category term="Google assistant" /><summary type="html">Recently, I moved into a new home and was setting up my Phillips Hue lights with my Google Home assistants around my house for convenience. I noticed a couple of hick-ups since the last time I did this. Logging into Phillips Hue app If the app does not ask you to hit the button on your bridge, your account already has a bridge associated with it. You can see what bridge is associated with your MeetHue account on the bridges page. Remove any older bridges you might have on your account and try logging into the Phillips Hue app again. Once complete, you should be able to link your Google Home assistant to your Phillips Hue app. Other House Keeping for security reasons You can cleanup how many apps have access to your account and how many other users have access to your bridge. If you see anything that your don’t recognize, remove it. After all, these apps and Hue account users can control the lights in your house. If you don’t know them, remove their access. In my case, the only users on my bridge are the family members in my house and the only apps I have are the Phillips Hue Android app (for mobile access remotely) and Google (assistant integration).</summary></entry><entry><title type="html">Efficient logback logging on JVM</title><link href="https://blog.karun.me/blog/2018/11/01/efficient-logback-logging-on-jvm/" rel="alternate" type="text/html" title="Efficient logback logging on JVM" /><published>2018-11-01T00:00:00+05:30</published><updated>2018-11-01T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2018/11/01/efficient-logback-logging-on-jvm</id><content type="html" xml:base="https://blog.karun.me/blog/2018/11/01/efficient-logback-logging-on-jvm/">&lt;p&gt;Efficient logging that doesn’t bring your application down is simple to setup but is often overlooked. Here are some quick tips on how to achieve exactly that
&lt;!-- more --&gt;&lt;/p&gt;

&lt;h2 id=&quot;async-logging&quot;&gt;Async Logging&lt;/h2&gt;

&lt;p&gt;Most applications these days should have a single (console) appender. This can be linked up with your log aggregator of choice. If your application cannot aggregate logs off the console stream, file is your next best alternative.&lt;/p&gt;

&lt;p&gt;Wrap each of your appenders with an async appender and add the async appender to your root logger.&lt;/p&gt;

&lt;p&gt;Every call to the logger creates a log event. In synchronous logging, that log event was processed and writes were made to all appender streams before the application continued. Since most stream writes involve I/O, this meant the application would wait for I/O before continuining thereby slowing it down. With async logging, the event gets pushed to a log level specific in memory queue. These events are processed and consumed by the appenders asynchronously. Since the application can continue after a log event has been published to the queue, asynchronous logging works quicker (as long as I/O is the long pole in the tent that is publishing log messages)&lt;/p&gt;

&lt;p&gt;Here’s a sample configuration:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&amp;gt;
    &amp;lt;file&amp;gt;myapp.log&amp;lt;/file&amp;gt;
    &amp;lt;encoder&amp;gt;
      &amp;lt;pattern&amp;gt;%logger{35} - %msg%n&amp;lt;/pattern&amp;gt;
    &amp;lt;/encoder&amp;gt;
  &amp;lt;/appender&amp;gt;

  &amp;lt;appender name=&quot;ASYNC-FILE&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&amp;gt;
    &amp;lt;appender-ref ref=&quot;FILE&quot; /&amp;gt;
    &amp;lt;queueSize&amp;gt;1024&amp;lt;/queueSize&amp;gt;
    &amp;lt;neverBlock&amp;gt;false&amp;lt;/neverBlock&amp;gt;
  &amp;lt;/appender&amp;gt;

  &amp;lt;root&amp;gt;
    &amp;lt;appender-ref ref=&quot;ASYNC-FILE&quot; /&amp;gt;
  &amp;lt;/root&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Every queue has a configurable depth. The depth of the queue is based on how much memory you have and expected ratio in rates of messages coming in through the application and the messages being published through the I/O bottleneck.&lt;/p&gt;

&lt;p&gt;If you hit max queue depth on either the &lt;code&gt;WARN&lt;/code&gt; or &lt;code&gt;ERROR&lt;/code&gt; queues, further statements for those levels become synchronous.&lt;/p&gt;

&lt;p&gt;If you hit more than 80% of the max queue depth on any other level, the system will start dropping log statements (due to &lt;code&gt;discardingThreshold=20&lt;/code&gt; by default and &lt;code&gt;neverBlock=true&lt;/code&gt;). Therefore, under high load, you can lose &lt;code&gt;INFO&lt;/code&gt;, &lt;code&gt;DEBUG&lt;/code&gt; and &lt;code&gt;TRACE&lt;/code&gt; log messages. This behaviour is acceptable for most cases except specific critical statements (like audit logs). For such cases, you can add asynchronous appenders that are allowed to block.&lt;/p&gt;

&lt;p&gt;The percentage of depth after which messages are dropped is configurable. You can make info/debug logs synchronous at 100% too if needed by changing the &lt;code&gt;neverBlock=false&lt;/code&gt; (which is the default behaviour).&lt;/p&gt;

&lt;p&gt;All of this information is available on &lt;a href=&quot;https://logback.qos.ch/manual/appenders.html#AsyncAppender&quot;&gt;logback’s documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;writing-log-statements&quot;&gt;Writing log statements&lt;/h2&gt;

&lt;p&gt;Async logs only work more efficiently because the production of events is synchronous (and hopefully a quick task) and the processing of events (which requires IO) is a slow task.&lt;/p&gt;

&lt;p&gt;However if production of log messages takes &lt;strong&gt;long time&lt;/strong&gt;, async logging will not make things better. When you’re printing a large amount of data or if the creation of the log message is an expensive operation, use the following kind of log statement&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;// style 1: java string interpolation; inefficient and hard to read :P
logger.info(&quot;Large object value was &quot; + largeObject1 + &quot; and long operation printed &quot; + largeObject2.longOperation())
// style 2: scala string interpolation; inefficient but easy to read
logger.info(s&quot;Large object value was $largeObject1 and long operation printed ${largeObject2.longOperation()}&quot;)
// style 3: logback based string interpolation; efficient but inconvenient to read
logger.info(&quot;Large object value was {} and long operation printed {}&quot;, largeObject1, largeObject2.longOperation())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While the scala interpolation (style 2) is the easiest to read, we should only do it when the objects being printed are small (small-ish strings or primitives).&lt;/p&gt;

&lt;p&gt;Rule of thumb:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For quick statements, use style 2.&lt;/li&gt;
  &lt;li&gt;For large statements, use style 3 (sacrifices readability for efficiency)&lt;/li&gt;
  &lt;li&gt;Never use style 1 :P&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;using-lazylogging-as-opposed-to-creating-loggers-yourself&quot;&gt;Using LazyLogging as opposed to creating loggers yourself&lt;/h2&gt;

&lt;p&gt;Use &lt;a href=&quot;https://github.com/lightbend/scala-logging&quot;&gt;lazy logging&lt;/a&gt;. It internally uses loggers that wraps yours code (during compile time) with if checks to not process log statements if the specific log level doesn’t need to be printed (&lt;a href=&quot;https://github.com/lightbend/scala-logging/blob/master/src/main/scala/com/typesafe/scalalogging/LoggerMacro.scala#L44&quot;&gt;using macros&lt;/a&gt;). Worried about performance due to extra if conditions? You shouldn’t. Modern processors contain black magic called &lt;a href=&quot;https://stackoverflow.com/questions/11227809/why-is-it-faster-to-process-a-sorted-array-than-an-unsorted-array&quot;&gt;branch prediction&lt;/a&gt; that reduce the effect of statements such as this to be effectively nothing.&lt;/p&gt;

&lt;p&gt;IMO, every scala project should use lazy logging. It’s &lt;a href=&quot;https://github.com/lightbend/scala-logging/blob/master/project/Dependencies.scala&quot;&gt;light on dependencies&lt;/a&gt; and has a nice implementation that makes your logging more efficient &lt;a href=&quot;https://github.com/lightbend/scala-logging/blob/master/src/main/scala/com/typesafe/scalalogging/LoggerMacro.scala&quot;&gt;run faster for fractionally slower compilation&lt;/a&gt;.&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Development" /><category term="Logging" /><category term="Logback" /><category term="Performance" /><category term="JVM" /><category term="Java" /><category term="Scala" /><summary type="html">Efficient logging that doesn’t bring your application down is simple to setup but is often overlooked. Here are some quick tips on how to achieve exactly that Async Logging Most applications these days should have a single (console) appender. This can be linked up with your log aggregator of choice. If your application cannot aggregate logs off the console stream, file is your next best alternative. Wrap each of your appenders with an async appender and add the async appender to your root logger. Every call to the logger creates a log event. In synchronous logging, that log event was processed and writes were made to all appender streams before the application continued. Since most stream writes involve I/O, this meant the application would wait for I/O before continuining thereby slowing it down. With async logging, the event gets pushed to a log level specific in memory queue. These events are processed and consumed by the appenders asynchronously. Since the application can continue after a log event has been published to the queue, asynchronous logging works quicker (as long as I/O is the long pole in the tent that is publishing log messages) Here’s a sample configuration: &amp;lt;configuration&amp;gt; &amp;lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&amp;gt; &amp;lt;file&amp;gt;myapp.log&amp;lt;/file&amp;gt; &amp;lt;encoder&amp;gt; &amp;lt;pattern&amp;gt;%logger{35} - %msg%n&amp;lt;/pattern&amp;gt; &amp;lt;/encoder&amp;gt; &amp;lt;/appender&amp;gt; &amp;lt;appender name=&quot;ASYNC-FILE&quot; class=&quot;ch.qos.logback.classic.AsyncAppender&quot;&amp;gt; &amp;lt;appender-ref ref=&quot;FILE&quot; /&amp;gt; &amp;lt;queueSize&amp;gt;1024&amp;lt;/queueSize&amp;gt; &amp;lt;neverBlock&amp;gt;false&amp;lt;/neverBlock&amp;gt; &amp;lt;/appender&amp;gt; &amp;lt;root&amp;gt; &amp;lt;appender-ref ref=&quot;ASYNC-FILE&quot; /&amp;gt; &amp;lt;/root&amp;gt; &amp;lt;/configuration&amp;gt; Every queue has a configurable depth. The depth of the queue is based on how much memory you have and expected ratio in rates of messages coming in through the application and the messages being published through the I/O bottleneck. If you hit max queue depth on either the WARN or ERROR queues, further statements for those levels become synchronous. If you hit more than 80% of the max queue depth on any other level, the system will start dropping log statements (due to discardingThreshold=20 by default and neverBlock=true). Therefore, under high load, you can lose INFO, DEBUG and TRACE log messages. This behaviour is acceptable for most cases except specific critical statements (like audit logs). For such cases, you can add asynchronous appenders that are allowed to block. The percentage of depth after which messages are dropped is configurable. You can make info/debug logs synchronous at 100% too if needed by changing the neverBlock=false (which is the default behaviour). All of this information is available on logback’s documentation. Writing log statements Async logs only work more efficiently because the production of events is synchronous (and hopefully a quick task) and the processing of events (which requires IO) is a slow task. However if production of log messages takes long time, async logging will not make things better. When you’re printing a large amount of data or if the creation of the log message is an expensive operation, use the following kind of log statement // style 1: java string interpolation; inefficient and hard to read :P logger.info(&quot;Large object value was &quot; + largeObject1 + &quot; and long operation printed &quot; + largeObject2.longOperation()) // style 2: scala string interpolation; inefficient but easy to read logger.info(s&quot;Large object value was $largeObject1 and long operation printed ${largeObject2.longOperation()}&quot;) // style 3: logback based string interpolation; efficient but inconvenient to read logger.info(&quot;Large object value was {} and long operation printed {}&quot;, largeObject1, largeObject2.longOperation()) While the scala interpolation (style 2) is the easiest to read, we should only do it when the objects being printed are small (small-ish strings or primitives). Rule of thumb: For quick statements, use style 2. For large statements, use style 3 (sacrifices readability for efficiency) Never use style 1 :P Using LazyLogging as opposed to creating loggers yourself Use lazy logging. It internally uses loggers that wraps yours code (during compile time) with if checks to not process log statements if the specific log level doesn’t need to be printed (using macros). Worried about performance due to extra if conditions? You shouldn’t. Modern processors contain black magic called branch prediction that reduce the effect of statements such as this to be effectively nothing. IMO, every scala project should use lazy logging. It’s light on dependencies and has a nice implementation that makes your logging more efficient run faster for fractionally slower compilation.</summary></entry><entry><title type="html">The Science in the Art of the Showcase (for distributed teams)</title><link href="https://blog.karun.me/blog/2018/07/03/the-science-in-the-art-of-the-showcase-for-distributed-teams/" rel="alternate" type="text/html" title="The Science in the Art of the Showcase (for distributed teams)" /><published>2018-07-03T00:00:00+05:30</published><updated>2018-07-03T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2018/07/03/the-science-in-the-art-of-the-showcase-for-distributed-teams</id><content type="html" xml:base="https://blog.karun.me/blog/2018/07/03/the-science-in-the-art-of-the-showcase-for-distributed-teams/">&lt;p&gt;&lt;a href=&quot;https://shreedamani.wordpress.com/tag/agile-showcase/&quot;&gt;Showcases&lt;/a&gt; are a key part of our agile ceremonies. We showcase our work to our stakeholders for feedback at the end of every iteration. And as with every presentation, I believe there is a Science in the Art of the Showcase (for distributed teams).&lt;/p&gt;

&lt;p&gt;On one of our recent teams, our showcases had challenges. Each of these challenges is a &lt;a href=&quot;http://www.extremeprogramming.org/values.html&quot;&gt;piece of feedback&lt;/a&gt;. We added structure to our showcases by running it like &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_theatre_personnel&quot;&gt;a theatre recording TV shows&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This isn’t revolutionary stuff. This is an attempt at defining a structure that should make it easier to organize showcases based off a check-list.&lt;/em&gt;&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;role&quot;&gt;Role&lt;/h2&gt;

&lt;h3 id=&quot;the-master-of-ceremonies&quot;&gt;The Master of Ceremonies&lt;/h3&gt;

&lt;p&gt;The MC is the face of the operations. They are responsible to dessiminate information and keep the crowd engaged. This means that the person should have context about what goes on and how to handle the different failures around client infra (skype issues, VDI issues etc).&lt;/p&gt;

&lt;h4 id=&quot;best-practices-for-mcs&quot;&gt;Best practices for MCs&lt;/h4&gt;

&lt;p&gt;Running commentary: Always keep speaking. Is there an issue? Keep the show rolling. Be transparent. Your support (folks below) will keep feeding you information when necessary.&lt;/p&gt;

&lt;h3 id=&quot;the-stagehand&quot;&gt;The Stagehand&lt;/h3&gt;

&lt;p&gt;This is the magician that controls the lighting on stage. This person actually runs the slides and the demos ensuring everything is smooth&lt;/p&gt;

&lt;h4 id=&quot;best-practices-for-stagehands&quot;&gt;Best practices for Stagehands&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Practice your demos repeatedly till it’s muscle memory&lt;/li&gt;
  &lt;li&gt;Ensure the demo windows are already prepared with data entry. Avoid copy pasting unless it cannot be avoided.&lt;/li&gt;
  &lt;li&gt;Ensure the content on the screen is visible on the media the stakeholders consume it on. If the stakeholders get together in a room and look at a screen projected on the screen or a big TV, please ensure that the font size is readable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-conductor&quot;&gt;The Conductor&lt;/h3&gt;

&lt;p&gt;This is the person who runs the show. This person is responsible to stay on the demo co-ordination chat and spot issues and handle them before they become a thing. This person is also responsible to give instant feedback to people running the showcase when needed.&lt;/p&gt;

&lt;h4 id=&quot;best-practices-for-the-conductor&quot;&gt;Best practices for The Conductor&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ensure you have an eye on the demo co-ordination chat. Delegate replying to another window if required&lt;/li&gt;
  &lt;li&gt;Ensure the MC is providing running commentary&lt;/li&gt;
  &lt;li&gt;Step in only if it is absolutely required&lt;/li&gt;
  &lt;li&gt;Keep an eye out for schedule&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-theatre-tech&quot;&gt;The Theatre Tech&lt;/h3&gt;

&lt;p&gt;The person who watches the logs and statuses for the services involved in the demo. If there is anything going wrong, talk to the conductor immediately.&lt;/p&gt;

&lt;h4 id=&quot;best-practices-for-the-theatre-techs&quot;&gt;Best practices for the Theatre Techs&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Have appropriate windows ready to perform the tasks you might need to in a hurry (bouncing services)&lt;/li&gt;
  &lt;li&gt;Have windows showing instance health&lt;/li&gt;
  &lt;li&gt;Have log window opens&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-timekeeper&quot;&gt;The Timekeeper&lt;/h3&gt;

&lt;p&gt;This person is in the room (with clients) and is responsible to keep time. If the discussion goes off, it is your responsibility to cut the discussion off and setup a followup discussion.&lt;/p&gt;

&lt;p&gt;If the clients are in multiple locations, have a timekeeper per location. Might be the conductor when available in a location.&lt;/p&gt;

&lt;h3 id=&quot;the-scribes&quot;&gt;The Scribes&lt;/h3&gt;

&lt;p&gt;Multiple people taking notes and sharing them after the demo. They are responsible to pick up body queues from the people around them and take notes on follow up discussions that we need to have.&lt;/p&gt;

&lt;h4 id=&quot;best-practices-for-thescribe&quot;&gt;Best practices for The Scribe&lt;/h4&gt;

&lt;p&gt;Be active on a demo co-ordination chat channel and provide instantaneous feedback from different locations. This helps the conductor get more information and is key to their effectiveness.&lt;/p&gt;

&lt;h3 id=&quot;the-playwright&quot;&gt;The Playwright&lt;/h3&gt;

&lt;p&gt;This person is primarily responsible for the content of the showcase.&lt;/p&gt;

&lt;p&gt;The content of a showcase should be like a TV show. A major milestone/deliverable is like a season and should have an overarching story (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Story_arc&quot;&gt;narrative arc&lt;/a&gt;). Each showcase is like an episode and should have a subsection of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Story_arc&quot;&gt;narrative arc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The way &lt;a href=&quot;https://presentationpatterns.com/glossary/#narrativearc&quot;&gt;Presentation Patterns&lt;/a&gt; book describes narrative arcs in presentations is true about showcases&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;del&gt;Presentations&lt;/del&gt; Showcases are a form of storytelling; don’t ignore a few thousand years of oratory history. A Narrative Arc is a common trope; organizing your &lt;del&gt;presentation&lt;/del&gt; showcase in a similar way leverages your audience’s lifetime of story listening experience.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;execution&quot;&gt;Execution&lt;/h2&gt;

&lt;p&gt;Know the people on your team. Identify which team members can do what roles. Invest in and groom people for roles based on their interest, it’s a growth opportunity.&lt;/p&gt;

&lt;h3 id=&quot;prep-work-for-the-venue&quot;&gt;Prep work for the venue&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;If your client site requires you to book rooms, do so as far out in advance as possible.&lt;/li&gt;
  &lt;li&gt;If your team is distributed, make sure the room has a good VC with a computer you can use to run the demo. Ensure your laptop can easily connect to the VC equipment in the room.&lt;/li&gt;
  &lt;li&gt;Know your venue and plan your seating. Presenters closer to the screen. Stakeholders in clean view of the screen and the presenters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;prep-on-the-day&quot;&gt;Prep on the day&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Sign up for roles based on your skills&lt;/li&gt;
  &lt;li&gt;Do multiple dry runs&lt;/li&gt;
  &lt;li&gt;Show up to the room 20 minutes before the start of the meeting. Set it up.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Karun Japhet</name></author><category term="Agile Practices" /><category term="showcase" /><category term="distributed teams" /><summary type="html">Showcases are a key part of our agile ceremonies. We showcase our work to our stakeholders for feedback at the end of every iteration. And as with every presentation, I believe there is a Science in the Art of the Showcase (for distributed teams). On one of our recent teams, our showcases had challenges. Each of these challenges is a piece of feedback. We added structure to our showcases by running it like a theatre recording TV shows. This isn’t revolutionary stuff. This is an attempt at defining a structure that should make it easier to organize showcases based off a check-list. Role The Master of Ceremonies The MC is the face of the operations. They are responsible to dessiminate information and keep the crowd engaged. This means that the person should have context about what goes on and how to handle the different failures around client infra (skype issues, VDI issues etc). Best practices for MCs Running commentary: Always keep speaking. Is there an issue? Keep the show rolling. Be transparent. Your support (folks below) will keep feeding you information when necessary. The Stagehand This is the magician that controls the lighting on stage. This person actually runs the slides and the demos ensuring everything is smooth Best practices for Stagehands Practice your demos repeatedly till it’s muscle memory Ensure the demo windows are already prepared with data entry. Avoid copy pasting unless it cannot be avoided. Ensure the content on the screen is visible on the media the stakeholders consume it on. If the stakeholders get together in a room and look at a screen projected on the screen or a big TV, please ensure that the font size is readable. The Conductor This is the person who runs the show. This person is responsible to stay on the demo co-ordination chat and spot issues and handle them before they become a thing. This person is also responsible to give instant feedback to people running the showcase when needed. Best practices for The Conductor Ensure you have an eye on the demo co-ordination chat. Delegate replying to another window if required Ensure the MC is providing running commentary Step in only if it is absolutely required Keep an eye out for schedule The Theatre Tech The person who watches the logs and statuses for the services involved in the demo. If there is anything going wrong, talk to the conductor immediately. Best practices for the Theatre Techs Have appropriate windows ready to perform the tasks you might need to in a hurry (bouncing services) Have windows showing instance health Have log window opens The Timekeeper This person is in the room (with clients) and is responsible to keep time. If the discussion goes off, it is your responsibility to cut the discussion off and setup a followup discussion. If the clients are in multiple locations, have a timekeeper per location. Might be the conductor when available in a location. The Scribes Multiple people taking notes and sharing them after the demo. They are responsible to pick up body queues from the people around them and take notes on follow up discussions that we need to have. Best practices for The Scribe Be active on a demo co-ordination chat channel and provide instantaneous feedback from different locations. This helps the conductor get more information and is key to their effectiveness. The Playwright This person is primarily responsible for the content of the showcase. The content of a showcase should be like a TV show. A major milestone/deliverable is like a season and should have an overarching story (aka narrative arc). Each showcase is like an episode and should have a subsection of the narrative arc. The way Presentation Patterns book describes narrative arcs in presentations is true about showcases Presentations Showcases are a form of storytelling; don’t ignore a few thousand years of oratory history. A Narrative Arc is a common trope; organizing your presentation showcase in a similar way leverages your audience’s lifetime of story listening experience. Execution Know the people on your team. Identify which team members can do what roles. Invest in and groom people for roles based on their interest, it’s a growth opportunity. Prep work for the venue If your client site requires you to book rooms, do so as far out in advance as possible. If your team is distributed, make sure the room has a good VC with a computer you can use to run the demo. Ensure your laptop can easily connect to the VC equipment in the room. Know your venue and plan your seating. Presenters closer to the screen. Stakeholders in clean view of the screen and the presenters. Prep on the day Sign up for roles based on your skills Do multiple dry runs Show up to the room 20 minutes before the start of the meeting. Set it up.</summary></entry><entry><title type="html">Upgrade everything in brew</title><link href="https://blog.karun.me/blog/2017/10/19/upgrade-everything-in-brew/" rel="alternate" type="text/html" title="Upgrade everything in brew" /><published>2017-10-19T00:00:00+05:30</published><updated>2017-10-19T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2017/10/19/upgrade-everything-in-brew</id><content type="html" xml:base="https://blog.karun.me/blog/2017/10/19/upgrade-everything-in-brew/">&lt;p&gt;&lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; is a the missing package manager for Mac OS. &lt;a href=&quot;https://caskroom.github.io/&quot;&gt;Brew cask&lt;/a&gt; extends &lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; and brings its elegance, simplicity, and speed to Mac OS applications and large binaries alike.&lt;/p&gt;

&lt;p&gt;If you’re using these tools and would like to upgrade all of the applications you have, run the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;brew update &amp;amp;&amp;amp; brew upgrade &amp;amp;&amp;amp; (brew cask outdated | cut -f 1 -d &quot; &quot; | xargs brew cask reinstall) &amp;amp;&amp;amp; brew cleanup
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;breaking-it-down&quot;&gt;Breaking it down&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Update brew with information from the latest taps: &lt;code&gt;brew update&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Upgrade apps in brew: &lt;code&gt;brew upgrade&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Update brew cask apps: &lt;code&gt;brew cask outdated | cut -f 1 -d &quot; &quot; | xargs brew cask reinstall&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Find outdated cask apps: &lt;code&gt;brew cask outdated&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Cut out the app names: &lt;code&gt;cut -f 1 -d &quot; &quot;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Upgrade brew cask apps: &lt;code&gt;xargs brew cask reinstall&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Remove installers for brew apps (to release disk space): &lt;code&gt;brew cleanup&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: &lt;code&gt;brew cask cleanup&lt;/code&gt; is now deprecated.&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Tutorials" /><category term="Brew" /><category term="Mac OS" /><category term="Automation" /><summary type="html">Homebrew is a the missing package manager for Mac OS. Brew cask extends Homebrew and brings its elegance, simplicity, and speed to Mac OS applications and large binaries alike. If you’re using these tools and would like to upgrade all of the applications you have, run the following command. brew update &amp;amp;&amp;amp; brew upgrade &amp;amp;&amp;amp; (brew cask outdated | cut -f 1 -d &quot; &quot; | xargs brew cask reinstall) &amp;amp;&amp;amp; brew cleanup Breaking it down Update brew with information from the latest taps: brew update Upgrade apps in brew: brew upgrade Update brew cask apps: brew cask outdated | cut -f 1 -d &quot; &quot; | xargs brew cask reinstall Find outdated cask apps: brew cask outdated Cut out the app names: cut -f 1 -d &quot; &quot; Upgrade brew cask apps: xargs brew cask reinstall Remove installers for brew apps (to release disk space): brew cleanup Note: brew cask cleanup is now deprecated.</summary></entry><entry><title type="html">Lombok usage in large enterprises</title><link href="https://blog.karun.me/blog/2017/08/13/lombok-usage-in-large-enterprises/" rel="alternate" type="text/html" title="Lombok usage in large enterprises" /><published>2017-08-13T00:00:00+05:30</published><updated>2017-08-13T00:00:00+05:30</updated><id>https://blog.karun.me/blog/2017/08/13/lombok-usage-in-large-enterprises</id><content type="html" xml:base="https://blog.karun.me/blog/2017/08/13/lombok-usage-in-large-enterprises/">&lt;h2 id=&quot;verbosity-of-java&quot;&gt;Verbosity of Java&lt;/h2&gt;

&lt;p&gt;Java is a verbose language. No one disputes it.&lt;/p&gt;

&lt;p&gt;Despite the clunky nature of the language syntax, it still is the language of choice in most enterprises. If you work in the services industry or are a technology consultant, chances are that you have to work with Java on a regular basis.&lt;/p&gt;

&lt;p&gt;If you’re also a fan of functional programming language and have worked any &lt;em&gt;modern&lt;/em&gt; programming language, you’ll recognize that Java’s syntax hinders your productivity because of the large amounts of boilerplate the language will generate. While newer JVM based lanaguages like &lt;a href=&quot;https://kotlinlang.org/docs/reference/data-classes.html&quot;&gt;Kotlin&lt;/a&gt; solve these problems in different ways, the open source community created &lt;a href=&quot;https://projectlombok.org/features/all&quot;&gt;Project Lombok&lt;/a&gt; to provide similar syntactic sugar in the world’s most popular enterprise programming language.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;what-is-lombok&quot;&gt;What is Lombok?&lt;/h2&gt;

&lt;p&gt;Lombok is a Java dependency that uses Java annotations to generate byte code straight into the class files during the compilation phase there by allowing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Boilerplate_code&quot;&gt;boilerplate code&lt;/a&gt; from your codebase to be significantly reduced.&lt;/p&gt;

&lt;p&gt;An example from the &lt;a href=&quot;http://jnb.ociweb.com/jnb/jnbJan2010.html#data&quot;&gt;Software Engineering Trends post from Jan 2010&lt;/a&gt; shows&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Data(staticConstructor=&amp;quot;of&amp;quot;)
public class Company {
  private final Person founder;
  private String name;
  private List&amp;lt;Person&amp;gt; employees;
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;would generate the same code as&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class Company {
  private final Person founder;
  private String name;
  private List&amp;lt;Person&amp;gt; employees;

  private Company(final Person founder) {
    this.founder = founder;
  }

  public static Company of(final Person founder) {
    return new Company(founder);
  }

  public Person getFounder() {
    return founder;
  }

  public String getName() {
    return name;
  }

  public void setName(final String name) {
    this.name = name;
  }

  public List&amp;lt;Person&amp;gt; getEmployees() {
    return employees;
  }

  public void setEmployees(final List&amp;lt;Person&amp;gt; employees) {
    this.employees = employees;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object o) {
    if (o == this) return true;
    if (o == null) return false;
    if (o.getClass() != this.getClass()) return false;
    final Company other = (Company)o;
    if (this.founder == null ? other.founder != null : !this.founder.equals(other.founder)) return false;
    if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false;
    if (this.employees == null ? other.employees != null : !this.employees.equals(other.employees)) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    final int PRIME = 31;
    int result = 1;
    result = result * PRIME + (this.founder == null ? 0 : this.founder.hashCode());
    result = result * PRIME + (this.name == null ? 0 : this.name.hashCode());
    result = result * PRIME + (this.employees == null ? 0 : this.employees.hashCode());
    return result;
  }

  @java.lang.Override
  public java.lang.String toString() {
      return &amp;quot;Company(founder=&amp;quot; + founder + &amp;quot;, name=&amp;quot; + name + &amp;quot;, employees=&amp;quot; + employees + &amp;quot;)&amp;quot;;
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;the-good&quot;&gt;The good&lt;/h3&gt;

&lt;p&gt;You shouldn’t have to write code that can be generated automatically. Of course, &lt;a href=&quot;https://www.jetbrains.com/help/idea/generating-getters-and-setters.html&quot;&gt;modern IDEs will do this for you with a few clicks of the keyboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re trying to optimize more than a few clicks though. Have a look at the equals method below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@java.lang.Override
  public boolean equals(final java.lang.Object o) {
    if (o == this) return true;
    if (o == null) return false;
    if (o.getClass() != this.getClass()) return false;
    final Company other = (Company)o;
    if (this.founder == null ? other.founder != null : !this.founder.equals(other.founder)) return false;
    if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false;
    if (this.employees == null ? other.employees != null : !this.employees.equals(other.employees)) return false;
    return true;
  }&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Is this a standard equals method (one where every field in the class is checked for equality)? Did we skip a field? Did we do a non standard check on one of the fields? Unless you go through the method line by line, there is no way to know.&lt;/p&gt;

&lt;p&gt;Generating code saves you the hassle of checking. If there is an annotation, you know the what the implementation will be (assuming you know how the framework works). If there’s code, chances are that it’s a non-standard implementation (or someone made a mistake).&lt;/p&gt;

&lt;h3 id=&quot;the-bad&quot;&gt;The bad&lt;/h3&gt;

&lt;p&gt;If you wish to check the generated code, you need an &lt;a href=&quot;https://plugins.jetbrains.com/plugin/7100-java-decompiler-intellij-plugin&quot;&gt;IDE that decompiles byte code&lt;/a&gt; or a &lt;a href=&quot;http://jd.benow.ca/&quot;&gt;tool that does the same&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If something’s wonky, debugging the issue might not be straight forward.&lt;/p&gt;

&lt;h3 id=&quot;the-downright-ugly&quot;&gt;The downright ugly&lt;/h3&gt;

&lt;p&gt;Modern IDEs like IntelliJ are &lt;a href=&quot;https://www.jetbrains.com/help/idea/refactoring-source-code.html&quot;&gt;built for refactoring&lt;/a&gt;. One of the most common refactoring options is the option to &lt;a href=&quot;https://www.jetbrains.com/help/idea/change-signature.html&quot;&gt;Change Signature&lt;/a&gt;. It’s an extremely useful option that allows you to reorder method (or constructor) parameters and the IDE takes care of the appropriate changes throughout the codebase.&lt;/p&gt;

&lt;p&gt;The order of the constructor parameters in a lombok-fied class is the order in which the parameters are declared. Changing this order changes the constructor signature.&lt;/p&gt;

&lt;p&gt;For a class with different parameter types, this is not a problem. Refactoring the following class&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Data
public class Company {
  private final Person founder;
  private String name;
  private List&amp;lt;Person&amp;gt; employees;
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;to the following signature&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Data
public class Company {
  private final Person founder;
  private List&amp;lt;Person&amp;gt; employees;
  private String name;
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;is not a problem. The usage of the constructor will fail to compile and provide feedback.&lt;/p&gt;

&lt;p&gt;If you have primitive types in your lombok-fied class, you have a problem. Refactoring the following class&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Data
public class Person {
  private final String employeeId;
  private final String firstName;
  private final String lastName;
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;to the following signature&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Data
public class Person {
  private final String firstName;
  private final String lastName;
  private final String employeeId;
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;will provide no feedback. The code will compile and set &lt;code&gt;employeeId&lt;/code&gt;s to &lt;code&gt;firstName&lt;/code&gt;s, &lt;code&gt;firstName&lt;/code&gt;s to &lt;code&gt;lastName&lt;/code&gt;s and &lt;code&gt;lastName&lt;/code&gt;s to &lt;code&gt;employeeId&lt;/code&gt;s. If you don’t have tests on the behavior of the &lt;code&gt;Person&lt;/code&gt; class, you won’t notice this issue until it’s too late. Hopefully, you don’t have &lt;a href=&quot;https://blog.karun.me/blog/2016/02/28/commonly-made-mistakes-in-unit-testing/&quot;&gt;tests for a data container with no behavior&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;where-is-lombok-appropriate&quot;&gt;Where is Lombok appropriate?&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Do you have a project where you have a strict set of contributors?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;because you’ll have to walk them through the rules of appropriate usage of lombok&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do your contributors understand Lombok well and how it works?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;because you will have unexpected defects due to refactoring if they don’t&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do your contributors understand how to properly unit test and do they understand the automation test pyramid?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;appropriate high level testing could catch functional defects. you don’t want unit tests checking constructors and getters&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do you have strict code quality control?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;without a way to check for inappropriate usage of lombok, defects can very easily creep in&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Is the team willing to invest time and effort into training new team members about Lombok and potential downsides?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;your learnings have to be passed to every future member of the codebase&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do most of your models use &lt;a href=&quot;https://www.martinfowler.com/bliki/ValueObject.html&quot;&gt;value objects&lt;/a&gt; and avoid primitives?&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;because reordering non-primitive fields will lead to compile exceptions providing feedback&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If your team can answer &lt;em&gt;yes&lt;/em&gt; to all of the above, you should use Lombok.&lt;/p&gt;

&lt;p&gt;I must admit, most large teams can’t answer &lt;em&gt;yes&lt;/em&gt; to all of the questions. Have you &lt;a href=&quot;https://try.kotlinlang.org/&quot;&gt;considered using Kotlin instead&lt;/a&gt;? :)&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Development" /><category term="Java" /><category term="Lombok" /><category term="Enterprise" /><category term="Code Cleanliness" /><summary type="html">Verbosity of Java Java is a verbose language. No one disputes it. Despite the clunky nature of the language syntax, it still is the language of choice in most enterprises. If you work in the services industry or are a technology consultant, chances are that you have to work with Java on a regular basis. If you’re also a fan of functional programming language and have worked any modern programming language, you’ll recognize that Java’s syntax hinders your productivity because of the large amounts of boilerplate the language will generate. While newer JVM based lanaguages like Kotlin solve these problems in different ways, the open source community created Project Lombok to provide similar syntactic sugar in the world’s most popular enterprise programming language. What is Lombok? Lombok is a Java dependency that uses Java annotations to generate byte code straight into the class files during the compilation phase there by allowing the boilerplate code from your codebase to be significantly reduced. An example from the Software Engineering Trends post from Jan 2010 shows @Data(staticConstructor=&amp;quot;of&amp;quot;) public class Company { private final Person founder; private String name; private List&amp;lt;Person&amp;gt; employees; } would generate the same code as public class Company { private final Person founder; private String name; private List&amp;lt;Person&amp;gt; employees; private Company(final Person founder) { this.founder = founder; } public static Company of(final Person founder) { return new Company(founder); } public Person getFounder() { return founder; } public String getName() { return name; } public void setName(final String name) { this.name = name; } public List&amp;lt;Person&amp;gt; getEmployees() { return employees; } public void setEmployees(final List&amp;lt;Person&amp;gt; employees) { this.employees = employees; } @java.lang.Override public boolean equals(final java.lang.Object o) { if (o == this) return true; if (o == null) return false; if (o.getClass() != this.getClass()) return false; final Company other = (Company)o; if (this.founder == null ? other.founder != null : !this.founder.equals(other.founder)) return false; if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false; if (this.employees == null ? other.employees != null : !this.employees.equals(other.employees)) return false; return true; } @java.lang.Override public int hashCode() { final int PRIME = 31; int result = 1; result = result * PRIME + (this.founder == null ? 0 : this.founder.hashCode()); result = result * PRIME + (this.name == null ? 0 : this.name.hashCode()); result = result * PRIME + (this.employees == null ? 0 : this.employees.hashCode()); return result; } @java.lang.Override public java.lang.String toString() { return &amp;quot;Company(founder=&amp;quot; + founder + &amp;quot;, name=&amp;quot; + name + &amp;quot;, employees=&amp;quot; + employees + &amp;quot;)&amp;quot;; } } The good You shouldn’t have to write code that can be generated automatically. Of course, modern IDEs will do this for you with a few clicks of the keyboard We’re trying to optimize more than a few clicks though. Have a look at the equals method below: @java.lang.Override public boolean equals(final java.lang.Object o) { if (o == this) return true; if (o == null) return false; if (o.getClass() != this.getClass()) return false; final Company other = (Company)o; if (this.founder == null ? other.founder != null : !this.founder.equals(other.founder)) return false; if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false; if (this.employees == null ? other.employees != null : !this.employees.equals(other.employees)) return false; return true; } Is this a standard equals method (one where every field in the class is checked for equality)? Did we skip a field? Did we do a non standard check on one of the fields? Unless you go through the method line by line, there is no way to know. Generating code saves you the hassle of checking. If there is an annotation, you know the what the implementation will be (assuming you know how the framework works). If there’s code, chances are that it’s a non-standard implementation (or someone made a mistake). The bad If you wish to check the generated code, you need an IDE that decompiles byte code or a tool that does the same. If something’s wonky, debugging the issue might not be straight forward. The downright ugly Modern IDEs like IntelliJ are built for refactoring. One of the most common refactoring options is the option to Change Signature. It’s an extremely useful option that allows you to reorder method (or constructor) parameters and the IDE takes care of the appropriate changes throughout the codebase. The order of the constructor parameters in a lombok-fied class is the order in which the parameters are declared. Changing this order changes the constructor signature. For a class with different parameter types, this is not a problem. Refactoring the following class @Data public class Company { private final Person founder; private String name; private List&amp;lt;Person&amp;gt; employees; } to the following signature @Data public class Company { private final Person founder; private List&amp;lt;Person&amp;gt; employees; private String name; } is not a problem. The usage of the constructor will fail to compile and provide feedback. If you have primitive types in your lombok-fied class, you have a problem. Refactoring the following class @Data public class Person { private final String employeeId; private final String firstName; private final String lastName; } to the following signature @Data public class Person { private final String firstName; private final String lastName; private final String employeeId; } will provide no feedback. The code will compile and set employeeIds to firstNames, firstNames to lastNames and lastNames to employeeIds. If you don’t have tests on the behavior of the Person class, you won’t notice this issue until it’s too late. Hopefully, you don’t have tests for a data container with no behavior. Where is Lombok appropriate? Do you have a project where you have a strict set of contributors? because you’ll have to walk them through the rules of appropriate usage of lombok Do your contributors understand Lombok well and how it works? because you will have unexpected defects due to refactoring if they don’t Do your contributors understand how to properly unit test and do they understand the automation test pyramid? appropriate high level testing could catch functional defects. you don’t want unit tests checking constructors and getters Do you have strict code quality control? without a way to check for inappropriate usage of lombok, defects can very easily creep in Is the team willing to invest time and effort into training new team members about Lombok and potential downsides? your learnings have to be passed to every future member of the codebase Do most of your models use value objects and avoid primitives? because reordering non-primitive fields will lead to compile exceptions providing feedback If your team can answer yes to all of the above, you should use Lombok. I must admit, most large teams can’t answer yes to all of the questions. Have you considered using Kotlin instead? :)</summary></entry><entry><title type="html">Hosting blogs for 1¢ a month</title><link href="https://blog.karun.me/blog/2016/12/05/hosting-blogs-for-1-cent-a-month/" rel="alternate" type="text/html" title="Hosting blogs for 1¢ a month" /><published>2016-12-05T02:04:56+05:30</published><updated>2016-12-05T02:04:56+05:30</updated><id>https://blog.karun.me/blog/2016/12/05/hosting-blogs-for-1-cent-a-month</id><content type="html" xml:base="https://blog.karun.me/blog/2016/12/05/hosting-blogs-for-1-cent-a-month/">&lt;p&gt;If you’re a dev and you self host your blog, I’d love to hear why. Why do you self host blogs? For most simple blogs in this day and age, migration to a static site like &lt;a href=&quot;https://wordpress.org/plugins/jekyll-exporter/&quot;&gt;Jekyll&lt;/a&gt; or &lt;a href=&quot;https://jason.pureconcepts.net/2013/01/migrating-wordpress-octopress/&quot;&gt;Octopress&lt;/a&gt; is pretty easy. I did this &lt;a href=&quot;https://blog.karun.me/blog/2015/11/28/movement-to-cybershark/&quot;&gt;a while back&lt;/a&gt;. This can be followed up by asking &lt;a href=&quot;https://davidwalsh.name/hosting-website-amazon-s3&quot;&gt;Amazon S3&lt;/a&gt; to host your website. You can even &lt;a href=&quot;https://blog.karun.me/blog/2015/02/01/forced-https-on-your-website-with-cloudflare/&quot;&gt;get cloudflare to front the SSL for free&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Why? S3 is free for the first year. Even post that period, my bills have been &amp;lt;$0.02/month which is a &lt;strong&gt;99.951% reduction in cost&lt;/strong&gt;.&lt;/p&gt;

&lt;!-- more --&gt;
&lt;h2 id=&quot;continuous-delivery&quot;&gt;Continuous delivery&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.karun.me/assets/images/uploads/https-blog-on-s3.jpg&quot; alt=&quot;HTTPs blog on S3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://snap-ci.com/&quot;&gt;Snap CI&lt;/a&gt; is will integrate with your publically accessible GitHub repositories for free and trigger builds on commit! Connect to your github repository and get it to compile your markdown into html. &lt;a href=&quot;https://docs.snap-ci.com/deployments/aws-deployments/aws-s3-deployments/&quot;&gt;Deploying to S3&lt;/a&gt; is a piece of cake. Congratulations on having continuous delivery for your blog!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.karun.me/blog/2015/02/01/forced-https-on-your-website-with-cloudflare/&quot;&gt;Cloudflare provides the free SSL&lt;/a&gt; and Amazon S3 provides the &lt;a href=&quot;https://aws.amazon.com/s3/pricing/&quot;&gt;near free hosting&lt;/a&gt;. A few cents a month to host your entire website is a good deal!&lt;/p&gt;

&lt;p&gt;I’ve been on S3 for a year now and I couldn’t be happier!&lt;/p&gt;

&lt;p&gt;** Goodbye Servers, Welcome S3 **&lt;/p&gt;</content><author><name>Karun Japhet</name></author><category term="Events" /><category term="news" /><category term="serverless" /><summary type="html">If you’re a dev and you self host your blog, I’d love to hear why. Why do you self host blogs? For most simple blogs in this day and age, migration to a static site like Jekyll or Octopress is pretty easy. I did this a while back. This can be followed up by asking Amazon S3 to host your website. You can even get cloudflare to front the SSL for free. Why? S3 is free for the first year. Even post that period, my bills have been &amp;lt;$0.02/month which is a 99.951% reduction in cost. Continuous delivery Snap CI is will integrate with your publically accessible GitHub repositories for free and trigger builds on commit! Connect to your github repository and get it to compile your markdown into html. Deploying to S3 is a piece of cake. Congratulations on having continuous delivery for your blog! Cloudflare provides the free SSL and Amazon S3 provides the near free hosting. A few cents a month to host your entire website is a good deal! I’ve been on S3 for a year now and I couldn’t be happier! ** Goodbye Servers, Welcome S3 **</summary></entry></feed>